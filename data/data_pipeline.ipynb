{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fui1hYK9mDgZ"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/xx-langchain-chunking.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/langchain/handbook/xx-langchain-chunking.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOt2ceUEmDgg"
      },
      "source": [
        "## Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E10PCQfemDgh"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain tiktoken matplotlib seaborn tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9lvKHBtmDgi"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from pydantic import BaseModel, Field\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "\n",
        "class Document(BaseModel): \n",
        "    page_content: str\n",
        "    metadata: dict = Field(default_factory=dict)\n",
        "\n",
        "class ReadDocLoader():\n",
        "    def __init__(self, path: str):\n",
        "        self.file_path = path\n",
        "\n",
        "    def load(self):\n",
        "         #Load documents\n",
        "        def _clean_data(data: str) -> str:\n",
        "            soup = BeautifulSoup(data, \"html.parser\")\n",
        "            text = soup.get_text().strip()\n",
        "            # Remove extra spaces and newlines\n",
        "            text = re.sub(r\"\\s+\", \" \", text)\n",
        "            # Join sentences together\n",
        "            sentences = re.split(r\"(?<=[.!?]) +\", text)\n",
        "            text = \" \".join(sentences).strip()\n",
        "            return text\n",
        "        \n",
        "        docs = []\n",
        "        for p in Path(self.file_path).rglob(\"*\"):\n",
        "            if p.is_dir():\n",
        "                continue\n",
        "            # Open the HTML file and read its contents\n",
        "            with open(p, \"r\") as f:\n",
        "                text = _clean_data(f.read())\n",
        "            # Clean the HTML data and create an instance of the Document class\n",
        "            metadata = {\"source\": str(p)}\n",
        "            docs.append(Document(page_content=text, metadata=metadata))\n",
        "        # Print the page content\n",
        "        return docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rP57p_xomDgk"
      },
      "outputs": [],
      "source": [
        "loader = ReadDocLoader('saved_pages')\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1L8mu5g-mDgl"
      },
      "outputs": [],
      "source": [
        "docs[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-PWxZvWmDgl"
      },
      "source": [
        "We access the plaintext page content like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paAyzlFjmDgm"
      },
      "outputs": [],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUpO-FjMmDgm"
      },
      "outputs": [],
      "source": [
        "print(docs[5].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qGjg_9KmDgm"
      },
      "source": [
        "We can also find the source of each document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e3_WiEnmDgm"
      },
      "outputs": [],
      "source": [
        "docs[5].metadata['source'].replace('saved_pages/', 'https://')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opX2Z5BlmDgn"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "# create the length function\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(\n",
        "        text,\n",
        "        disallowed_special=()\n",
        "    )\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QYVss2kmDgn"
      },
      "outputs": [],
      "source": [
        "tiktoken.encoding_for_model('gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CucQx0imDgo"
      },
      "outputs": [],
      "source": [
        "token_counts = [tiktoken_len(doc.page_content) for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78J9DojLmDgo"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"Min: {min(token_counts)}\n",
        "Avg: {int(sum(token_counts) / len(token_counts))}\n",
        "Max: {max(token_counts)}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKqYQvavmDgo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# set style and color palette for the plot\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(\"muted\")\n",
        "\n",
        "# create histogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(token_counts, kde=False, bins=50)\n",
        "\n",
        "# customize the plot info\n",
        "plt.title(\"Token Counts Histogram\")\n",
        "plt.xlabel(\"Token Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncitL6LEmDgp"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,  # number of tokens overlap between chunks\n",
        "    length_function=tiktoken_len,\n",
        "    separators=['\\n\\n', '\\n', ' ', '']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM9zTUTmmDgp"
      },
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(docs[5].page_content)\n",
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8Xt3_a7mDgp"
      },
      "outputs": [],
      "source": [
        "tiktoken_len(chunks[0]), tiktoken_len(chunks[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QELRRRojmDgq"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "m = hashlib.md5()  # this will convert URL into unique ID\n",
        "\n",
        "url = docs[5].metadata['source'].replace('saved_pages/', 'https://')\n",
        "print(url)\n",
        "\n",
        "# convert URL to unique ID\n",
        "m.update(url.encode('utf-8'))\n",
        "uid = m.hexdigest()[:12]\n",
        "print(uid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Mk3YQNnmDgq"
      },
      "outputs": [],
      "source": [
        "data = [\n",
        "    {\n",
        "        'id': f'{uid}-{i}',\n",
        "        'text': chunk,\n",
        "        'source': url\n",
        "    } for i, chunk in enumerate(chunks)\n",
        "]\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4I9yDf-mDgr"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "documents = []\n",
        "\n",
        "for doc in tqdm(docs):\n",
        "    url = doc.metadata['source'].replace('saved_pages', 'https://')\n",
        "    m.update(url.encode('utf-8'))\n",
        "    uid = m.hexdigest()[:12]\n",
        "    chunks = text_splitter.split_text(doc.page_content)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        documents.append({\n",
        "            'id': f'{uid}-{i}',\n",
        "            'text': chunk,\n",
        "            'source': url\n",
        "        })\n",
        "\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0e6auzjmDgr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('train.jsonl', 'w') as f:\n",
        "    for doc in documents:\n",
        "        f.write(json.dumps(doc) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKOYgWTlmDgr"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "\n",
        "with open('train.jsonl', 'r') as f:\n",
        "    for line in f:\n",
        "        documents.append(json.loads(line))\n",
        "\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNiay_MmmDgr"
      },
      "outputs": [],
      "source": [
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "documents = [{\n",
        "    'id': doc['id'],\n",
        "    'text': doc['text'],\n",
        "    'metadata': {'url': doc['source']}\n",
        "} for doc in documents]\n",
        "\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BEARER_TOKEN = os.environ.get(\"BEARER_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {BEARER_TOKEN}\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "batch_size = 100\n",
        "endpoint_url = \"https://plankton-app-6cv28.ondigitalocean.app\"\n",
        "s = requests.Session()\n",
        "\n",
        "# we setup a retry strategy to retry on 5xx errors\n",
        "retries = Retry(\n",
        "    total=5,  # number of retries before raising error\n",
        "    backoff_factor=0.1,\n",
        "    status_forcelist=[500, 502, 503, 504]\n",
        ")\n",
        "s.mount('http://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "for i in tqdm(range(0, len(documents), batch_size)):\n",
        "    i_end = min(len(documents), i+batch_size)\n",
        "    # make post request that allows up to 5 retries\n",
        "    res = s.post(\n",
        "        f\"{endpoint_url}/upsert\",\n",
        "        headers=headers,\n",
        "        json={\n",
        "            \"documents\": documents[i:i_end]\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "queries = [\n",
        "    {'query': \"What are the rate limits for Exchange/Pro?\"},\n",
        "]\n",
        "\n",
        "res = requests.post(\n",
        "    f\"{endpoint_url}/query\",\n",
        "    headers=headers,\n",
        "    json={\n",
        "        'queries': queries\n",
        "    }\n",
        ")\n",
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(res.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for query_result in res.json()['results']:\n",
        "    query = query_result['query']\n",
        "    answers = []\n",
        "    scores = []\n",
        "    for result in query_result['results']:\n",
        "        answers.append(result['text'])\n",
        "        scores.append(round(result['score'], 2))\n",
        "    print(\"-\"*70+\"\\n\"+query+\"\\n\\n\"+\"\\n\".join([f\"{s}: {a}\" for a, s in zip(answers, scores)])+\"\\n\"+\"-\"*70+\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
